% ============================================================================
% CVPR Research Proposal: Next-Generation 3D Gaussian Splatting
% Based on Taming 3DGS Implementation and RTX 5080 Optimizations
% ============================================================================
% Author: Research Team
% Date: October 26, 2025
% Target Conference: CVPR 2026
% ============================================================================

\documentclass[11pt,letterpaper]{article}

% Packages
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{natbib}

% Custom commands
\newcommand{\method}[1]{\textbf{#1}}
\newcommand{\highlight}[1]{\textcolor{blue}{\textbf{#1}}}
\newcommand{\contribution}[1]{\textcolor{red}{\textbf{[#1]}}}

\title{\textbf{Research Proposal: Next-Generation 3D Gaussian Splatting} \\
\large Towards CVPR 2026 Publication}

\author{Research Team \\ Based on Taming 3DGS + RTX 5080 Optimizations}
\date{October 26, 2025}

\begin{document}

\maketitle

\begin{abstract}
This document outlines \textbf{six novel research directions} for advancing 3D Gaussian Splatting (3DGS) technology, building upon our successful implementation of Taming 3DGS on consumer hardware (NVIDIA RTX 5080, 16GB VRAM). We identify key limitations in current approaches and propose concrete implementations targeting CVPR 2026 publication. Our proposals range from \textbf{memory-efficient training} for democratizing 3DGS research, to \textbf{neural importance networks} for learned densification strategies, and \textbf{uncertainty-aware rendering} for robust novel view synthesis. Each direction includes detailed implementation plans, expected contributions, and resource requirements.
\end{abstract}

% ============================================================================
\section{Current State: Our Implementation}
% ============================================================================

\subsection{Achievements}

\textbf{Platform:} Successfully adapted Taming 3DGS for NVIDIA RTX 5080 (Blackwell architecture, 16GB VRAM)

\textbf{Results:}
\begin{itemize}[leftmargin=*]
    \item \textbf{Budget Mode:} 13/13 datasets (2h 2m, avg PSNR 25.93 dB)
    \item \textbf{Big Mode:} 13/13 datasets (18.8M Gaussians, 27.66 GB total)
    \item \textbf{Memory Efficiency:} $3\times$ VRAM reduction (48GB → 16GB)
\end{itemize}

\subsection{Key Technical Contributions (Already Implemented)}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Blackwell Architecture Support:} PyTorch 2.9+ with CUDA 12.8, sm\_120 compatibility
    \item \textbf{Memory Optimizations:}
    \begin{itemize}
        \item CPU data device for image storage
        \item Expandable memory segments (PYTORCH\_CUDA\_ALLOC\_CONF)
        \item Adaptive densification intervals (300 vs 100 for A6000)
        \item Batch k-NN processing for initialization
    \end{itemize}
    \item \textbf{Algorithm Fixes:}
    \begin{itemize}
        \item Safe array indexing for variable densification intervals
        \item Budget control bounds checking
        \item Loop iteration robustness
    \end{itemize}
\end{enumerate}

\subsection{Codebase Understanding}

We have comprehensive documentation covering:
\begin{itemize}[leftmargin=*]
    \item Complete Python codebase architecture (35+ files documented)
    \item Data flow for training, rendering, and evaluation
    \item Core algorithms: importance scoring, densification, budget scheduling
    \item All utilities, dataset readers, and rendering pipeline
\end{itemize}

\textbf{Repository:} \url{https://github.com/humansensinglab/taming-3dgs}

% ============================================================================
\section{Research Gaps \& Opportunities}
% ============================================================================

\subsection{Current Limitations in 3DGS}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Hardware Accessibility:} Requires expensive workstation GPUs (A6000: \$4,500)
    \item \textbf{Hand-Crafted Importance:} 12 manually tuned coefficients for Gaussian scoring
    \item \textbf{Static Training Strategy:} Fixed resolution, no progressive refinement
    \item \textbf{No Uncertainty Estimation:} Cannot quantify reconstruction confidence
    \item \textbf{Memory Inefficiency:} Big mode uses 27GB for 13 scenes (2.1GB per scene avg)
    \item \textbf{Limited Dynamic Support:} Cannot handle moving/deformable objects
\end{enumerate}

\subsection{CVPR Relevance}

\textbf{Hot Topics at CVPR 2024-2025:}
\begin{itemize}[leftmargin=*]
    \item Neural radiance fields and 3D reconstruction
    \item Efficient neural representations
    \item Learned optimization strategies
    \item Uncertainty in deep learning
    \item Real-time rendering
    \item Dynamic scene understanding
\end{itemize}

Our proposals align with \textbf{all six} trending areas.

% ============================================================================
\section{Proposed Research Directions}
% ============================================================================

% ----------------------------------------------------------------------------
\subsection{Direction 1: Memory-Efficient 3DGS for Consumer Hardware \highlight{[HIGH PRIORITY]}}
% ----------------------------------------------------------------------------

\subsubsection{Motivation}

\textbf{Problem:} Original 3DGS requires 48GB VRAM (RTX A6000), limiting accessibility to research labs with expensive hardware.

\textbf{Our Insight:} We successfully trained on 16GB RTX 5080 through adaptive strategies. This can be formalized into a general framework.

\subsubsection{Proposed Method: \method{AdaptiveGS}}

\textbf{Core Idea:} Dynamically adjust training hyperparameters based on available VRAM.

\begin{algorithm}
\caption{AdaptiveGS Training}
\begin{algorithmic}
\STATE \textbf{Input:} VRAM budget $B$, scene complexity $C$
\STATE \textbf{Initialize:} Compute optimal densification interval $d = f(B, C)$
\FOR{iteration $i = 1$ to $N$}
    \STATE Monitor VRAM usage $U_i$
    \IF{$U_i > 0.9 \cdot B$}
        \STATE Increase densification interval: $d \leftarrow d + \Delta d$
        \STATE Reduce batch size or image resolution
    \ELSIF{$U_i < 0.5 \cdot B$}
        \STATE Decrease interval: $d \leftarrow \max(d - \Delta d, d_{min})$
    \ENDIF
    \STATE Perform densification every $d$ iterations
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Novel Contributions:}
\begin{enumerate}[leftmargin=*]
    \item \contribution{C1} Learned memory-to-quality trade-off function $f(B, C)$
    \item \contribution{C2} Dynamic interval scheduling based on real-time VRAM monitoring
    \item \contribution{C3} Hierarchical Gaussian representation (LOD-like structure)
    \item \contribution{C4} Comprehensive benchmark on consumer GPUs (RTX 3080, 4090, 5080, 5090)
\end{enumerate}

\textbf{Implementation Plan:}
\begin{itemize}[leftmargin=*]
    \item \textbf{Week 1-2:} Implement VRAM monitoring and adaptive interval scheduling
    \item \textbf{Week 3-4:} Design hierarchical Gaussian structure
    \item \textbf{Week 5-6:} Train function $f(B, C)$ on synthetic data with varying complexity
    \item \textbf{Week 7-8:} Benchmark on MipNeRF360, Tanks\&Temples with multiple GPUs
    \item \textbf{Week 9-10:} Ablation studies, writing
\end{itemize}

\textbf{Expected Results:}
\begin{itemize}[leftmargin=*]
    \item Train high-quality scenes on 8GB GPUs (RTX 4060 Ti)
    \item $<5\%$ quality degradation vs 48GB baseline
    \item 10$\times$ cost reduction for hardware
\end{itemize}

\textbf{Dataset Requirements:} MipNeRF360 (9 scenes), Tanks\&Temples (2 scenes), DeepBlending (2 scenes) — \textit{already available}

% ----------------------------------------------------------------------------
\subsection{Direction 2: Neural Importance Networks \highlight{[HIGHEST NOVELTY]}}
% ----------------------------------------------------------------------------

\subsubsection{Motivation}

\textbf{Problem:} Current importance scoring uses 12 hand-tuned coefficients:
\begin{align*}
\text{score} &= \sum_{i=1}^{12} w_i \cdot \text{normalize}(\text{feature}_i) \\
\text{where } &w_i \in \{\text{grad, opac, depth, radii, scale, dist, loss, ...}\}
\end{align*}

This is \textbf{scene-dependent} and \textbf{non-optimal}. Different scenes may need different importance weights.

\subsubsection{Proposed Method: \method{LearnedDensify}}

Replace hand-crafted scoring with a small MLP that learns to predict Gaussian importance.

\textbf{Architecture:}
\begin{align*}
\text{Input:} \quad &\mathbf{f}_g = [\text{gradient}, \text{opacity}, \text{depth}, \text{radii}, \text{scale}] \in \mathbb{R}^5 \\
&\mathbf{f}_p = [\text{dist\_accum}, \text{loss\_accum}, \text{blend}, \text{count}] \in \mathbb{R}^4 \\
\text{Network:} \quad &\text{MLP}_\theta: \mathbb{R}^9 \rightarrow \mathbb{R}^1 \\
\text{Output:} \quad &s_i = \text{MLP}_\theta([\mathbf{f}_g^i; \mathbf{f}_p^i]) \quad \text{(importance score)}
\end{align*}

\textbf{Training Strategy:} Meta-learning approach
\begin{enumerate}[leftmargin=*]
    \item Train on multiple scenes simultaneously
    \item Optimize for final reconstruction quality (PSNR, SSIM, LPIPS)
    \item Use MAML or similar for fast adaptation to new scenes
\end{enumerate}

\textbf{Novel Contributions:}
\begin{enumerate}[leftmargin=*]
    \item \contribution{C1} First learned importance function for 3DGS densification
    \item \contribution{C2} Meta-learned network generalizing across scene types
    \item \contribution{C3} Differentiable densification strategy
    \item \contribution{C4} Attention mechanism for multi-view importance aggregation
\end{enumerate}

\textbf{Implementation Plan:}
\begin{itemize}[leftmargin=*]
    \item \textbf{Week 1-2:} Implement lightweight MLP (2-3 layers, 64 hidden dims)
    \item \textbf{Week 3-4:} Integrate into training loop with gradient flow
    \item \textbf{Week 5-6:} Implement meta-learning (train on 10 scenes, test on 3)
    \item \textbf{Week 7-8:} Add attention mechanism for camera-specific importance
    \item \textbf{Week 9-12:} Extensive experiments, ablations, comparison with hand-crafted
\end{itemize}

\textbf{Expected Results:}
\begin{itemize}[leftmargin=*]
    \item +1-2 dB PSNR improvement over hand-crafted coefficients
    \item Zero-shot generalization to unseen scene types
    \item Faster convergence (fewer iterations needed)
\end{itemize}

\textbf{Key Insight:} This is similar to recent work on learned optimizers (e.g., VeLO, L2O) but applied to 3D reconstruction.

% ----------------------------------------------------------------------------
\subsection{Direction 3: Progressive Multi-Resolution Training \highlight{[EFFICIENCY]}}}
% ----------------------------------------------------------------------------

\subsubsection{Motivation}

\textbf{Problem:} Current approach uses fixed resolution throughout training, which is wasteful:
\begin{itemize}[leftmargin=*]
    \item Early iterations: Gaussians are coarsely placed, high-res images add no benefit
    \item Later iterations: Fine details emerge, need high resolution
\end{itemize}

\subsubsection{Proposed Method: \method{ProgressiveGS}}

Coarse-to-fine training schedule:
\begin{align*}
\text{Iteration } i: \quad \text{Resolution} &= R_0 \cdot 2^{\lfloor i / T \rfloor} \\
\text{where } R_0 &= \text{base resolution (e.g., images\_8)} \\
T &= \text{schedule parameter (e.g., 5000 iters)}
\end{align*}

\textbf{Example Schedule:}
\begin{itemize}[leftmargin=*]
    \item Iterations 0-5000: \texttt{images\_8} (12.5\% resolution)
    \item Iterations 5000-10000: \texttt{images\_4} (25\%)
    \item Iterations 10000-15000: \texttt{images\_2} (50\%)
    \item Iterations 15000-30000: \texttt{images} (100\%)
\end{itemize}

\textbf{Novel Contributions:}
\begin{enumerate}[leftmargin=*]
    \item \contribution{C1} Principled resolution scheduling for 3DGS
    \item \contribution{C2} Adaptive schedule based on reconstruction error
    \item \contribution{C3} Joint optimization of resolution and densification
    \item \contribution{C4} 2-3$\times$ speedup with minimal quality loss
\end{enumerate}

\textbf{Implementation Plan:}
\begin{itemize}[leftmargin=*]
    \item \textbf{Week 1:} Modify data loader to support dynamic resolution switching
    \item \textbf{Week 2-3:} Implement fixed schedules (linear, exponential, step-wise)
    \item \textbf{Week 4-5:} Adaptive scheduling based on validation PSNR
    \item \textbf{Week 6-7:} Study interaction with densification intervals
    \item \textbf{Week 8:} Benchmarking and comparison
\end{itemize}

\textbf{Expected Results:}
\begin{itemize}[leftmargin=*]
    \item 2-3$\times$ training speedup
    \item Same or better final quality (early coarse guidance helps)
    \item Reduced VRAM footprint during early training
\end{itemize}

% ----------------------------------------------------------------------------
\subsection{Direction 4: Uncertainty-Aware 3DGS \highlight{[ROBUSTNESS]}}
% ----------------------------------------------------------------------------

\subsubsection{Motivation}

\textbf{Problem:} Current 3DGS provides point estimates without uncertainty quantification. This is problematic for:
\begin{itemize}[leftmargin=*]
    \item Novel view quality assessment (which views are reliable?)
    \item Active view selection (where should we capture more images?)
    \item Safety-critical applications (autonomous driving, medical imaging)
\end{itemize}

\subsubsection{Proposed Method: \method{UncertaintyGS}}

Extend Gaussians to encode uncertainty over their parameters.

\textbf{Probabilistic Gaussian:}
\begin{align*}
\text{Position: } &\boldsymbol{\mu}_{\text{pos}} \sim \mathcal{N}(\mathbf{x}, \boldsymbol{\Sigma}_{\text{pos}}) \\
\text{Opacity: } &\alpha \sim \text{Beta}(a, b) \\
\text{Scale: } &\mathbf{s} \sim \text{LogNormal}(\boldsymbol{\mu}_s, \boldsymbol{\sigma}_s) \\
\text{Color (SH): } &\mathbf{c} \sim \mathcal{N}(\boldsymbol{\mu}_c, \boldsymbol{\Sigma}_c)
\end{align*}

\textbf{Rendering with Uncertainty:}
\begin{itemize}[leftmargin=*]
    \item Sample $K$ parameter sets from distributions
    \item Render $K$ images
    \item Aggregate: Mean for RGB, Variance for uncertainty map
\end{itemize}

\textbf{Loss Function:}
\begin{align*}
\mathcal{L} &= \mathcal{L}_{\text{recon}} + \lambda_1 \mathcal{L}_{\text{KL}} + \lambda_2 \mathcal{L}_{\text{calib}} \\
\text{where } \mathcal{L}_{\text{KL}} &= \text{KL}(q(\theta) \| p(\theta)) \quad \text{(regularization)} \\
\mathcal{L}_{\text{calib}} &= \| \sigma^2 - (\text{GT} - \mu)^2 \|^2 \quad \text{(calibration)}
\end{align*}

\textbf{Novel Contributions:}
\begin{enumerate}[leftmargin=*]
    \item \contribution{C1} First uncertainty quantification for 3DGS
    \item \contribution{C2} Efficient sampling-based rendering (dropout-like)
    \item \contribution{C3} Calibrated uncertainty estimates
    \item \contribution{C4} Active view selection using uncertainty maps
\end{enumerate}

\textbf{Implementation Plan:}
\begin{itemize}[leftmargin=*]
    \item \textbf{Week 1-2:} Extend GaussianModel to include variance parameters
    \item \textbf{Week 3-4:} Implement sampling-based rendering (K=5 samples)
    \item \textbf{Week 5-6:} Add KL and calibration losses
    \item \textbf{Week 7-8:} Test on novel view synthesis with uncertainty visualization
    \item \textbf{Week 9-10:} Active learning experiments (select next best views)
\end{itemize}

\textbf{Expected Results:}
\begin{itemize}[leftmargin=*]
    \item Uncertainty correlates with reconstruction error (Pearson $r > 0.7$)
    \item Active view selection reduces required views by 30-40\%
    \item Minimal computational overhead ($<$20\% slower training)
\end{itemize}

% ----------------------------------------------------------------------------
\subsection{Direction 5: Compression and Pruning \highlight{[DEPLOYMENT]}}
% ----------------------------------------------------------------------------

\subsubsection{Motivation}

\textbf{Problem:} Our Big Mode uses 2.1GB per scene on average (27.66GB / 13 scenes). This is too large for:
\begin{itemize}[leftmargin=*]
    \item Mobile/edge deployment
    \item Streaming applications
    \item Large-scale scene databases
\end{itemize}

\textbf{Comparison:}
\begin{itemize}[leftmargin=*]
    \item \textbf{3DGS Big Mode:} 2.1GB/scene (our measurement)
    \item \textbf{NeRF (full):} $\sim$5MB/scene
    \item \textbf{Compressed NeRF:} $\sim$1MB/scene
\end{itemize}

Target: \textbf{10-100$\times$ compression with $<$1 dB PSNR loss}

\subsubsection{Proposed Method: \method{CompressedGS}}

Multi-stage compression pipeline:

\textbf{Stage 1: Pruning}
\begin{itemize}[leftmargin=*]
    \item Remove Gaussians with $\alpha < \tau$ (low opacity)
    \item Remove Gaussians with $\text{visibility\_count} < k$ (rarely rendered)
    \item Expected: 30-50\% reduction
\end{itemize}

\textbf{Stage 2: Quantization}
\begin{itemize}[leftmargin=*]
    \item Positions: Float32 → Float16 (2$\times$ reduction)
    \item SH coefficients: Learned codebook quantization (4-8 bits)
    \item Opacity/Scale/Rotation: Logarithmic quantization (8 bits)
    \item Expected: 4-8$\times$ reduction
\end{itemize}

\textbf{Stage 3: Entropy Coding}
\begin{itemize}[leftmargin=*]
    \item Arithmetic coding on quantized values
    \item Spatial clustering for better compression
    \item Expected: 1.5-2$\times$ reduction
\end{itemize}

\textbf{Overall: } $0.5 \times 0.2 \times 0.6 = 0.06$ → \textbf{16.7$\times$ compression} \\
Result: 2.1GB → \textbf{125MB per scene}

\textbf{Novel Contributions:}
\begin{enumerate}[leftmargin=*]
    \item \contribution{C1} Joint pruning and quantization framework
    \item \contribution{C2} Importance-aware quantization (different bits for different Gaussians)
    \item \contribution{C3} Neural compression for SH coefficients
    \item \contribution{C4} Real-time decompression for rendering
\end{enumerate}

\textbf{Implementation Plan:}
\begin{itemize}[leftmargin=*]
    \item \textbf{Week 1-2:} Implement pruning based on opacity and visibility
    \item \textbf{Week 3-4:} Quantization strategies (uniform, k-means, learned)
    \item \textbf{Week 5-6:} Train neural compressor for SH coefficients
    \item \textbf{Week 7-8:} Entropy coding and file format design
    \item \textbf{Week 9-10:} End-to-end evaluation on all datasets
\end{itemize}

\textbf{Expected Results:}
\begin{itemize}[leftmargin=*]
    \item 10-20$\times$ compression
    \item $<$1 dB PSNR loss
    \item Real-time decompression ($>$30 FPS on RTX 5080)
\end{itemize}

% ----------------------------------------------------------------------------
\subsection{Direction 6: Dynamic Scene Reconstruction \highlight{[EXTENSION]}}
% ----------------------------------------------------------------------------

\subsubsection{Motivation}

\textbf{Problem:} Current 3DGS is limited to \textbf{static scenes}. Many real-world applications require dynamic content:
\begin{itemize}[leftmargin=*]
    \item Human performance capture
    \item Sports broadcasting
    \item Autonomous driving
\end{itemize}

Recent work (D-3DGS, 4D-GS) tackles this but has limitations:
\begin{itemize}[leftmargin=*]
    \item High memory usage
    \item No temporal consistency enforcement
    \item Slow training
\end{itemize}

\subsubsection{Proposed Method: \method{DynamicGS}}

Extend Gaussians with temporal dimension using deformation fields.

\textbf{Time-Dependent Gaussian:}
\begin{align*}
\mathbf{x}(t) &= \mathbf{x}_0 + \mathbf{D}_{\text{pos}}(\mathbf{x}_0, t; \theta) \\
\mathbf{s}(t) &= \mathbf{s}_0 \cdot \exp(\mathbf{D}_{\text{scale}}(\mathbf{x}_0, t; \theta)) \\
\mathbf{q}(t) &= \text{normalize}(\mathbf{q}_0 + \mathbf{D}_{\text{rot}}(\mathbf{x}_0, t; \theta)) \\
\alpha(t) &= \sigma(\alpha_0 + \mathbf{D}_{\text{opac}}(\mathbf{x}_0, t; \theta))
\end{align*}

Where $\mathbf{D}$ are small MLPs (hash-encoded for efficiency).

\textbf{Key Innovation: Memory-Efficient Temporal Encoding}
\begin{itemize}[leftmargin=*]
    \item Use hash-grid encoding (like Instant-NGP) for time dimension
    \item Share deformation networks across all Gaussians
    \item Temporal smoothness regularization
\end{itemize}

\textbf{Novel Contributions:}
\begin{enumerate}[leftmargin=*]
    \item \contribution{C1} Memory-efficient dynamic 3DGS using hash encoding
    \item \contribution{C2} Temporal consistency losses for smoother motion
    \item \contribution{C3} Benchmark on consumer GPUs (first dynamic 3DGS on 16GB)
    \item \contribution{C4} Interactive editing of dynamic scenes
\end{enumerate}

\textbf{Implementation Plan:}
\begin{itemize}[leftmargin=*]
    \item \textbf{Week 1-3:} Implement deformation networks with tiny-cuda-nn
    \item \textbf{Week 4-5:} Integrate temporal dimension into rendering
    \item \textbf{Week 6-7:} Add temporal smoothness losses
    \item \textbf{Week 8-10:} Test on DNeRF, D-NeRF datasets
    \item \textbf{Week 11-12:} Capture custom dynamic scenes, benchmarking
\end{itemize}

\textbf{Expected Results:}
\begin{itemize}[leftmargin=*]
    \item Match D-3DGS quality with 2-3$\times$ less memory
    \item Train on RTX 5080 (16GB) vs original 48GB requirement
    \item Real-time rendering of dynamic scenes
\end{itemize}

\textbf{Dataset Requirements:}
\begin{itemize}[leftmargin=*]
    \item D-NeRF synthetic dataset (8 scenes) — publicly available
    \item DNeRF real dataset — publicly available
    \item Custom captures using phone camera (easy to obtain)
\end{itemize}

% ============================================================================
\section{Comparative Analysis \& Recommendation}
% ============================================================================

\begin{table}[h]
\centering
\caption{Comparison of Proposed Research Directions}
\small
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{Direction} & \textbf{Novelty} & \textbf{Impact} & \textbf{Difficulty} & \textbf{Timeline} & \textbf{CVPR Fit} \\
\midrule
1. AdaptiveGS & Medium & High & Low & 10 weeks & \textbf{Good} \\
2. LearnedDensify & \textbf{Very High} & \textbf{Very High} & \textbf{High} & 12 weeks & \textbf{Excellent} \\
3. ProgressiveGS & Medium & Medium & Low & 8 weeks & Good \\
4. UncertaintyGS & High & High & Medium & 10 weeks & \textbf{Excellent} \\
5. CompressedGS & Medium & High & Medium & 10 weeks & Good \\
6. DynamicGS & High & \textbf{Very High} & High & 12 weeks & \textbf{Excellent} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Recommended Strategy}

\textbf{Option A: Single Strong Paper (CVPR Main Track)} \\
\textit{Focus on Direction 2 (LearnedDensify) + Direction 4 (UncertaintyGS)}

\textbf{Rationale:}
\begin{itemize}[leftmargin=*]
    \item Highest novelty: First learned densification for 3DGS
    \item Natural synergy: Uncertainty helps guide densification
    \item Strong story: From hand-crafted to learned optimization
    \item Clear baseline: Compare against Taming 3DGS hand-tuned coefficients
\end{itemize}

\textbf{Title:} "Learning to Densify: Neural Importance Networks for Uncertainty-Aware 3D Gaussian Splatting"

\textbf{Timeline:} 20-24 weeks (5-6 months)

\vspace{1em}

\textbf{Option B: Multiple Papers (1 Main + 1-2 Workshops)} \\
\textit{Main: Direction 2 (LearnedDensify)} \\
\textit{Workshop 1: Direction 1 (AdaptiveGS)} \\
\textit{Workshop 2: Direction 5 (CompressedGS)}

\textbf{Rationale:}
\begin{itemize}[leftmargin=*]
    \item Maximize publication count
    \item AdaptiveGS good fit for "Efficient Deep Learning" workshop
    \item CompressedGS good fit for "Neural Compression" workshop
\end{itemize}

\vspace{1em}

\textbf{Option C: High-Risk, High-Reward (Direction 6)} \\
\textit{Focus solely on DynamicGS}

\textbf{Rationale:}
\begin{itemize}[leftmargin=*]
    \item Huge potential impact (new capability)
    \item Growing interest in dynamic scenes
    \item Our memory optimizations are perfect foundation
    \item Risk: Harder to implement, requires new datasets
\end{itemize}

% ============================================================================
\section{Detailed Implementation: LearnedDensify (Recommended)}
% ============================================================================

\subsection{Architecture Design}

\textbf{Network Structure:}

\begin{verbatim}
Input: [grad(3), opacity(1), depth(1), radii(1), scale(3),
        dist_accum(1), loss_accum(1), blend(1), count(1)]  # 13D

Layer 1: Linear(13 -> 64) + ReLU
Layer 2: Linear(64 -> 64) + ReLU
Layer 3: Linear(64 -> 32) + ReLU
Layer 4: Linear(32 -> 1) + Softplus  # Importance score > 0

Total Parameters: ~5K (tiny!)
\end{verbatim}

\textbf{Training Loop Integration:}

\begin{algorithm}
\caption{LearnedDensify Training}
\begin{algorithmic}
\STATE \textbf{Initialize:} GaussianModel $G$, ImportanceNet $I_\theta$
\FOR{iteration $i = 1$ to $N$}
    \STATE Render image, compute loss, backward
    \STATE Update Gaussian parameters
    \IF{$i \mod \text{densify\_interval} == 0$}
        \STATE Extract features: $\mathbf{F} = [\text{grad}, \text{opac}, \ldots]$
        \STATE Predict importance: $\mathbf{s} = I_\theta(\mathbf{F})$
        \STATE Densify using $\mathbf{s}$ (clone/split high-importance)
        \STATE \textcolor{red}{// Key: Backprop through densification to train $I_\theta$}
        \STATE Compute meta-loss: $\mathcal{L}_{\text{meta}} = \text{PSNR}_{t+k}$
        \STATE Update $\theta$ to maximize future quality
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Meta-Learning Strategy}

\textbf{Problem:} Cannot backprop through discrete sampling (multinomial).

\textbf{Solution:} Use straight-through estimator or Gumbel-Softmax.

\begin{align*}
\text{Forward: } &\text{sample} \sim \text{Multinomial}(\text{softmax}(\mathbf{s} / \tau)) \\
\text{Backward: } &\text{gradient flows through softmax}
\end{align*}

\textbf{Outer Loop (Meta-Learning):}
\begin{enumerate}[leftmargin=*]
    \item Sample $M$ scenes from training set
    \item For each scene:
    \begin{itemize}
        \item Inner loop: Train Gaussians for $k$ steps with current $I_\theta$
        \item Compute validation PSNR after $k$ steps
    \end{itemize}
    \item Update $I_\theta$ to maximize average validation PSNR
    \item Use MAML or Reptile for efficient meta-learning
\end{enumerate}

\subsection{Ablation Studies}

\textbf{Required Ablations:}
\begin{enumerate}[leftmargin=*]
    \item Hand-crafted vs learned importance (main comparison)
    \item Network size (32D, 64D, 128D hidden)
    \item Meta-learning vs single-scene training
    \item Different feature combinations (which features matter most?)
    \item Attention vs simple aggregation for multi-view
\end{enumerate}

\subsection{Expected Experimental Results}

\textbf{Quantitative:}
\begin{table}[h]
\centering
\caption{Expected Performance Improvement}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{PSNR} & \textbf{SSIM} & \textbf{LPIPS} \\
\midrule
Taming 3DGS (hand-crafted) & 25.93 & 0.762 & 0.223 \\
\textbf{LearnedDensify (ours)} & \textbf{27.1} (+1.2) & \textbf{0.791} (+0.03) & \textbf{0.198} (-0.025) \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Qualitative:}
\begin{itemize}[leftmargin=*]
    \item Better reconstruction of fine details (edges, textures)
    \item More efficient Gaussian placement (fewer "floaters")
    \item Faster convergence (reach same quality in 20k vs 30k iters)
\end{itemize}

% ============================================================================
\section{Resource Requirements}
% ============================================================================

\subsection{Hardware}

\textbf{Current Available:}
\begin{itemize}[leftmargin=*]
    \item NVIDIA RTX 5080 (16GB VRAM) — \textit{already have}
    \item CPU: Modern multi-core processor
    \item RAM: 64GB+ recommended
\end{itemize}

\textbf{Additional Recommended:}
\begin{itemize}[leftmargin=*]
    \item RTX 5090 (32GB VRAM) for big mode experiments — \$2,000
    \item Additional RTX 4090 for parallelization — \$1,800
    \item High-capacity SSD (4TB NVMe) for datasets — \$300
\end{itemize}

\textbf{Total Additional Cost:} \$4,100

\subsection{Software \& Datasets}

\textbf{Already Available:}
\begin{itemize}[leftmargin=*]
    \item Taming 3DGS codebase (fully functional)
    \item MipNeRF360 dataset (9 scenes)
    \item Tanks\&Temples dataset (2 scenes)
    \item DeepBlending dataset (2 scenes)
    \item PyTorch 2.9, CUDA 12.8
\end{itemize}

\textbf{Additional Needed:}
\begin{itemize}[leftmargin=*]
    \item For DynamicGS: D-NeRF dataset (free download)
    \item Meta-learning libraries: learn2learn, higher (free)
    \item Compression libraries: tiny-cuda-nn (free)
\end{itemize}

\subsection{Time Investment}

\textbf{For LearnedDensify + UncertaintyGS:}
\begin{itemize}[leftmargin=*]
    \item \textbf{Implementation:} 12-14 weeks (3-3.5 months)
    \item \textbf{Experiments:} 6-8 weeks (1.5-2 months)
    \item \textbf{Writing:} 4-6 weeks (1-1.5 months)
    \item \textbf{Total:} 22-28 weeks (5.5-7 months)
\end{itemize}

\textbf{CVPR 2026 Deadline:} Likely November 2025 \\
\textbf{Start Date:} Late October 2025 (now!) \\
\textbf{Available Time:} 6-7 months → \textbf{Feasible!}

% ============================================================================
\section{Risk Mitigation}
% ============================================================================

\subsection{Technical Risks}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Risk:} Learned importance network doesn't improve over hand-crafted

    \textit{Mitigation:} Start with simple linear model, gradually increase complexity. Even matching hand-crafted with learned is publishable (automation).

    \item \textbf{Risk:} Meta-learning is unstable or doesn't generalize

    \textit{Mitigation:} Fall back to single-scene learned importance. Still novel contribution.

    \item \textbf{Risk:} Uncertainty estimation adds too much overhead

    \textit{Mitigation:} Use dropout approximation (faster). Focus on uncertainty for densification guidance, not rendering.

    \item \textbf{Risk:} VRAM limitations on RTX 5080

    \textit{Mitigation:} We already solved this! Use CPU data device, adaptive intervals, etc.
\end{enumerate}

\subsection{Timeline Risks}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Risk:} Implementation takes longer than expected

    \textit{Mitigation:} Modular design. Each component (importance net, uncertainty) can stand alone.

    \item \textbf{Risk:} Experiments require too much compute time

    \textit{Mitigation:} Use progressive training (Direction 3) to speed up all experiments. Parallel training on multiple GPUs.

    \item \textbf{Risk:} CVPR deadline is sooner than expected

    \textit{Mitigation:} Have backup plan for workshop submission (CVPR workshops, ICCV workshops).
\end{enumerate}

% ============================================================================
\section{Success Metrics}
% ============================================================================

\subsection{Minimum Viable Contribution (MVC)}

For CVPR acceptance, we need \textbf{at least one} of:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Quantitative Improvement:} +0.5 dB PSNR over Taming 3DGS
    \item \textbf{Efficiency Gain:} 2$\times$ speedup or 2$\times$ memory reduction
    \item \textbf{Novel Capability:} Uncertainty estimation (first for 3DGS)
    \item \textbf{Strong Ablations:} Show learned importance is better than any fixed weighting
\end{enumerate}

\subsection{Target Metrics}

\textbf{For LearnedDensify:}
\begin{itemize}[leftmargin=*]
    \item PSNR: 26.5+ dB (vs 25.93 baseline)
    \item SSIM: 0.780+ (vs 0.762 baseline)
    \item LPIPS: $<$0.210 (vs 0.223 baseline)
    \item Convergence: Same quality in 20k vs 30k iterations
\end{itemize}

\textbf{For UncertaintyGS:}
\begin{itemize}[leftmargin=*]
    \item Uncertainty-error correlation: Pearson $r > 0.7$
    \item Calibration error: ECE $<$ 0.05
    \item Active learning: 30\% reduction in required views
\end{itemize}

% ============================================================================
\section{Publication Strategy}
% ============================================================================

\subsection{Target Venue: CVPR 2026}

\textbf{Why CVPR:}
\begin{itemize}[leftmargin=*]
    \item Top-tier venue (h-index: 389)
    \item Strong 3D vision track
    \item Recent 3DGS papers: "3D Gaussian Splatting" (SIGGRAPH 2023), "Taming 3DGS" (SIGGRAPH Asia 2024)
    \item Natural continuation of this line of work
\end{itemize}

\textbf{Timeline:}
\begin{itemize}[leftmargin=*]
    \item Submission deadline: Likely November 15, 2025
    \item Notification: February 2026
    \item Conference: June 2026
\end{itemize}

\subsection{Paper Structure (Tentative)}

\textbf{Title:} "Learning to Densify: Neural Importance Networks for 3D Gaussian Splatting"

\textbf{Abstract:} (150-200 words) \\
Introduce problem (hand-crafted importance), our solution (learned networks), key results (+1.2 dB PSNR).

\textbf{Sections:}
\begin{enumerate}[leftmargin=*]
    \item Introduction (1 page)
    \item Related Work (1 page)
    \item Method (3 pages)
    \begin{itemize}
        \item Background: 3DGS and Taming 3DGS
        \item Problem: Hand-crafted importance scoring
        \item Solution: ImportanceNet architecture
        \item Meta-learning strategy
        \item Optional: Uncertainty extension
    \end{itemize}
    \item Experiments (3 pages)
    \begin{itemize}
        \item Datasets and metrics
        \item Comparison with baselines
        \item Ablation studies
        \item Qualitative results
    \end{itemize}
    \item Conclusion (0.5 page)
\end{enumerate}

\textbf{Supplementary Material:}
\begin{itemize}[leftmargin=*]
    \item Implementation details
    \item Additional ablations
    \item Video results
    \item Code release (GitHub)
\end{itemize}

\subsection{Backup Plans}

\textbf{If CVPR main track is too competitive:}
\begin{enumerate}[leftmargin=*]
    \item CVPR Workshop: "Efficient Deep Learning for Computer Vision"
    \item ICCV 2025 (October deadline, December conference)
    \item 3DV 2026 (March deadline)
    \item ECCV 2026 (March deadline, October conference)
\end{enumerate}

% ============================================================================
\section{Conclusion \& Next Steps}
% ============================================================================

\subsection{Summary}

We have identified \textbf{six promising research directions} building on our successful Taming 3DGS + RTX 5080 implementation. The most promising for CVPR 2026:

\begin{enumerate}[leftmargin=*]
    \item \textbf{LearnedDensify:} Replace hand-crafted importance with learned network \\
    \textit{Novelty: Very High | Impact: Very High | Feasibility: High}

    \item \textbf{UncertaintyGS:} Add uncertainty quantification to 3DGS \\
    \textit{Novelty: High | Impact: High | Feasibility: Medium}
\end{enumerate}

\textbf{Recommended approach:} Combine both in single paper for maximum impact.

\subsection{Immediate Action Items}

\textbf{Week 1 (Now):}
\begin{enumerate}[leftmargin=*]
    \item Set up development branch: \texttt{feature/learned-importance}
    \item Implement basic ImportanceNet (3-layer MLP)
    \item Integrate into training loop (forward pass only)
    \item Sanity check: Can network predict reasonable importance scores?
\end{enumerate}

\textbf{Week 2-3:}
\begin{enumerate}[leftmargin=*]
    \item Implement gradient flow through densification (Gumbel-Softmax)
    \item Train on single scene (bicycle) with fixed learning rate
    \item Compare learned vs hand-crafted importance
    \item If promising, proceed to meta-learning
\end{enumerate}

\textbf{Week 4-6:}
\begin{enumerate}[leftmargin=*]
    \item Implement MAML/Reptile for meta-learning
    \item Train on 10 scenes, test on 3 held-out scenes
    \item Extensive ablations
\end{enumerate}

\textbf{Week 7-12:}
\begin{enumerate}[leftmargin=*]
    \item Add uncertainty extension
    \item Full benchmark on all 13 datasets
    \item Start writing paper
\end{enumerate}

\subsection{Decision Point}

\textbf{Go/No-Go Decision: Week 3}

After initial experiments, evaluate:
\begin{itemize}[leftmargin=*]
    \item Does learned importance match or exceed hand-crafted on \textit{any} scene?
    \item Is gradient flow working (losses decreasing)?
    \item Are there obvious failure modes?
\end{itemize}

\textbf{If Yes:} Proceed with full implementation \\
\textbf{If No:} Pivot to Direction 1 (AdaptiveGS) or Direction 5 (CompressedGS)

% ============================================================================
% References
% ============================================================================

\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{kerbl3Dgaussians}
Kerbl, B., Kopanas, G., Leimkühler, T., \& Drettakis, G. (2023).
\textit{3D Gaussian Splatting for Real-Time Radiance Field Rendering}.
ACM Transactions on Graphics (TOG), 42(4).

\bibitem{taming3dgs}
Mallick, S. S., Goel, R., Kerbl, B., Carrasco, F. V., Steinberger, M., \& De La Torre, F. (2024).
\textit{Taming 3DGS: High-Quality Radiance Fields with Limited Resources}.
SIGGRAPH Asia 2024.

\bibitem{d3dgs}
Yan, L., et al. (2024).
\textit{Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis}.
3DV 2024.

\bibitem{instantngp}
Müller, T., Evans, A., Schied, C., \& Keller, A. (2022).
\textit{Instant Neural Graphics Primitives with a Multiresolution Hash Encoding}.
ACM Transactions on Graphics (TOG), 41(4).

\bibitem{maml}
Finn, C., Abbeel, P., \& Levine, S. (2017).
\textit{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}.
ICML 2017.

\bibitem{uncertaintynerf}
Martin-Brualla, R., et al. (2021).
\textit{NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections}.
CVPR 2021.

\end{thebibliography}

\end{document}
